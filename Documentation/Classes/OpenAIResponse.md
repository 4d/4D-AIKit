# OpenAIResponse

The `OpenAIResponse` class represents a response object from the OpenAI Responses API.

<https://platform.openai.com/docs/api-reference/responses/object>

## Properties

| Property | Type | Description |
|----------|------|-------------|
| `id` | Text | Unique identifier for this Response |
| `object` | Text | The object type of this resource - always set to "response" |
| `model` | Text | Model ID used to generate the response |
| `created_at` | Real | Unix timestamp (in seconds) of when this Response was created |
| `status` | Text | The status of the response generation |
| `output` | Collection | An array of content items generated by the model |
| `instructions` | Variant | A system (or developer) message inserted into the model's context |
| `metadata` | Object | Set of 16 key-value pairs that can be attached to an object |
| `parallel_tool_calls` | Boolean | Whether to allow the model to run tool calls in parallel |
| `temperature` | Real | What sampling temperature was used |
| `tool_choice` | Variant | How the model selected which tool (or tools) to use |
| `tools` | Collection | An array of tools the model may call while generating a response |
| `top_p` | Real | Alternative to sampling with temperature, called nucleus sampling |
| `background` | Boolean | Whether to run the model response in the background |
| `max_output_tokens` | Integer | Upper bound for the number of tokens that can be generated |
| `max_tool_calls` | Integer | The maximum number of total calls to built-in tools |
| `previous_response_id` | Text | The unique ID of the previous response to the model |
| `prompt` | Object | Reference to a prompt template and its variables |
| `prompt_cache_key` | Text | Used by OpenAI to cache responses for similar requests |
| `reasoning` | Object | Configuration options for reasoning models |
| `safety_identifier` | Text | A stable identifier used to help detect users violating OpenAI's usage policies |
| `service_tier` | Text | Specifies the processing type used for serving the request |
| `text` | Object | Configuration options for a text response from the model |
| `top_logprobs` | Integer | Number of most likely tokens returned at each token position |
| `truncation` | Text | The truncation strategy used for the model response |
| `usage` | Object | Token usage details including input tokens, output tokens, and total tokens used |
| `user` | Text | Field being replaced by safety_identifier and prompt_cache_key |
| `error` | Object | An error object returned when the model fails to generate a Response |
| `incomplete_details` | Object | Details about why the response is incomplete |

## Computed Properties

| Property | Type | Description |
|----------|------|-------------|
| `output_text` | Text | Convenience property that aggregates all output_text items from the output list |
| `success` | Boolean | Boolean indicating whether the response was successful |

## Example

```4d
var $result : cs.OpenAIResponsesResult
$result:=$client.responses.create("Hello!"; $parameters)

If ($result.success)
    var $response : cs.OpenAIResponse
    $response:=$result.response
    
    // Access response properties
    ALERT("Response ID: "+$response.id)
    ALERT("Model: "+$response.model)
    ALERT("Status: "+$response.status)
    ALERT("Output: "+$response.output_text)
    
    // Check if response is successful
    If ($response.success)
        // Process the response
    End if
End if
```

## See also

- [OpenAIResponsesAPI](OpenAIResponsesAPI.md)
- [OpenAIResponsesResult](OpenAIResponsesResult.md)
- [OpenAIResponseOutputItem](OpenAIResponseOutputItem.md)
